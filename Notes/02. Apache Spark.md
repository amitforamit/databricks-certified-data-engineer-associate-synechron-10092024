# Apache Spark

> Apache Spark is an in-memory cluster computing framework (Distributed Processing Framework) designed to handle a wide range of big data workloads

1. High Performance Batch computation
2. Data Integration and ETL
3. Machine Learning Analytics
4. Real-stream processing
5. Graph Computation

**Important Points**

1. The main feature of Spark is its **In-Memory** cluster computing framework

## What is PySpark

> PySpark is the Python API for Apache Spark

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeJ2CizmweTFRRy1BR2nArd6q_81-zCbNgyzGR1e0CscU91S-GwJuf8rBhvPUfDZsKBwc8tqU1cD2AyRjr96_5QG96DmSdqrhWcQdftl4i5qzfbaxGIHIN_CAWwSJKe4HtAjTjZTdc0Y-cqrYyEmNkbV4w?key=uvmlVet7-pBAx-jz0PuzLA)

## Spark Ecosystem

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcmQiMmGeCZ5r-IDx7XBaT2cRWExsog9gfG_QmwrBoh-okpLyJ94FDU2UhaSu9OA7BHs4opkOw5zmDRbHK6M-BvMTsL4uEQC4HiTTKYpbcb2O1N5uAwKCghtMzBewKlqH6EaAqurfoKXzQxT9pRbneSPfs?key=uvmlVet7-pBAx-jz0PuzLA)

## Two Levels of API

1. Low Level API  -> RDD's
2. High Level API -> DataFrame API, Dataset API, SQL



## Spark Interactive Shell

> The spark interactive shell refers to the command line interface through which we can execute spark programs

1. Spark Shell (Scala)
2. PySpark Shell (Python)

1. Spark interactive shell creates two objects

   spark --> SparkSession

   sc --> SparkContext

2. Cluster Manager - local[*]

3. Every spark application will have a Web UI through which you can monitor the job.